{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import json\n",
                "import random"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "df = pd.read_csv(\"data/histo_feature.csv\")\n",
                "df = df.iloc[:,9:]\n",
                "df = df.drop(df[df[\"conclusion\"]==\"OTHER\"].index)\n",
                "df = df.drop(df[df[\"conclusion\"]==\"UNCLEAR\"].index)\n",
                "del df[\"datetime\"]\n",
                "df = df.dropna(axis=1, thresh=5)\n",
                "df.fillna(0, inplace=True)\n",
                "df = df.replace({0.25:1, 0.5:1, 0.75:1})"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "stat_per_diag = json.load(open(\"data/stat_per_diag.json\",\"rb\"))\n",
                "\n",
                "def random_term_list(dct):\n",
                "    term_list = []\n",
                "    for k, v in dct.items():\n",
                "        rand_val = random.random()\n",
                "        if rand_val <= v/100:\n",
                "            term_list.append(k)\n",
                "    return term_list\n",
                "\n",
                "def stat_per_diags(df, features_col):\n",
                "    stat_per_diag = {}\n",
                "    all_diag = list(set(df.conclusion.to_list()))\n",
                "    for i in all_diag:\n",
                "        ds = df[df[\"conclusion\"]==i][features_col].sum().sort_values(ascending=False)\n",
                "        nrow = len(df[df[\"conclusion\"]==i])\n",
                "        ds = ds / nrow * 100\n",
                "        stat_per_diag[i] = {}\n",
                "        stat_per_diag[i][\"n\"] = nrow\n",
                "        stat_per_diag[i][\"feature\"] = ds[ds>0].round().to_dict()\n",
                "    return stat_per_diag"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "synth_data_dict = {}\n",
                "for disease in [\"COM\",\"NM\",\"CNM\"]:\n",
                "    for patient in range(100):\n",
                "        synth_data_dict[disease+\"_\"+str(patient)] = []\n",
                "        synth_data_dict[disease+\"_\"+str(patient)].append(disease)\n",
                "        patient_term_list = random_term_list(stat_per_diag[disease][\"feature\"])\n",
                "        for i,value in enumerate(df.columns):\n",
                "            if value in patient_term_list:\n",
                "                synth_data_dict[disease+\"_\"+str(patient)].append(1)\n",
                "            elif value != \"conclusion\":\n",
                "                synth_data_dict[disease+\"_\"+str(patient)].append(0)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "df2 = pd.DataFrame.from_dict(synth_data_dict, orient='index', columns=df.columns)\n",
                "df2.to_csv(\"data/histo_fake_data.csv\", index=False)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "normal_mean = df.sum(axis=1).mean()\n",
                "normal_std = df.sum(axis=1).std()\n",
                "term_number = int(np.round(np.random.normal(loc=normal_mean, scale=normal_std)))\n",
                "print(term_number)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.metrics import plot_confusion_matrix, confusion_matrix\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.model_selection import learning_curve \n",
                "\n",
                "df = pd.read_csv(\"data/histo_fake_data.csv\")\n",
                "X, Y = df.iloc[:,1:],df.iloc[:,0]\n",
                "label_encoder = LabelEncoder()\n",
                "label_encoder = label_encoder.fit(Y)\n",
                "label_encoded_y = label_encoder.transform(Y)\n",
                "\n",
                "\n",
                "clf = SVC(class_weight=\"balanced\", probability=True, random_state=777)\n",
                "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=777)\n",
                "train_sizes, train_scores, test_scores = learning_curve(clf, X, label_encoded_y, shuffle=True, cv=cv, scoring=\"accuracy\")\n",
                "clf = clf.fit(X, label_encoded_y)\n",
                "\n",
                "print(\"Cross-Validation Scores:\")\n",
                "print(cross_val_score(clf, X, label_encoded_y, cv=cv, scoring=\"accuracy\"))\n",
                "plt.plot(range(5), cross_val_score(clf, X, label_encoded_y, cv=cv, scoring=\"accuracy\"), 'o-', color=\"r\",\n",
                "                 label=\"Cross-Validation Accuracy\")\n",
                "ylim = plt.ylim(0,1)\n",
                "xticks = plt.xticks(range(5))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "train_scores_mean = np.mean(train_scores, axis=1)\n",
                "test_scores_mean = np.mean(test_scores, axis=1)\n",
                "train_scores_std = np.std(train_scores, axis=1)\n",
                "test_scores_std = np.std(test_scores, axis=1)\n",
                "\n",
                "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
                "                 label=\"Training score\")\n",
                "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
                "                 label=\"Cross-validation score\")\n",
                "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
                "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
                "                         color=\"r\")\n",
                "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
                "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
                "                         color=\"g\")\n",
                "plt.ylim(0.75,1.01)\n",
                "plt.title(\"Learning Curve (SVM). Mean Accuracy +- 1 std\")\n",
                "plt.xlabel(\"Training Examples\")\n",
                "plt.ylabel(\"Accuracy Score\")\n",
                "plt.legend(loc=\"best\")\n",
                "plt.grid()\n",
                "plt.show()\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Target - Data DF\n",
                "df = pd.read_csv(\"data/histo_feature.csv\")\n",
                "df = df.iloc[:,9:]\n",
                "# Drop les OTHER pour l'instant (que 3 classes)\n",
                "df = df.drop(df[df[\"conclusion\"]==\"OTHER\"].index)\n",
                "df = df.drop(df[df[\"conclusion\"]==\"UNCLEAR\"].index)\n",
                "del df[\"datetime\"]\n",
                "# Enlever les col remplis de NaN ou avec moins de 5 valeur (annotations)\n",
                "df = df.dropna(axis=1, thresh=5)\n",
                "df.fillna(0, inplace=True)\n",
                "df = df.replace({0.25:1, 0.5:1, 0.75:1})\n",
                "# SÃ©parer les features des labels et onehot encoding des labels\n",
                "# NM:2, COM:1, UNCLEAR:4, CNM:0, OTHER:3\n",
                "X_test, Y_test = df.iloc[:,1:],df.iloc[:,0]\n",
                "label_encoder_test = LabelEncoder()\n",
                "label_encoder_test = label_encoder_test.fit(Y_test)\n",
                "label_encoded_y_test = label_encoder_test.transform(Y_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import plot_confusion_matrix\n",
                "y_predict = clf.predict(X_test)\n",
                "print(accuracy_score(label_encoded_y_test, y_predict))\n",
                "plot_confusion_matrix(clf, X_test, label_encoded_y_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "y_predict"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def stat_per_genes(df, features_col):\n",
                "    stat_per_gene = {}\n",
                "    all_genes = list(set(df.gene_diag.to_list()))\n",
                "    for i in all_genes:\n",
                "        ds = df[df[\"gene_diag\"]==i][features_col].sum().sort_values(ascending=False)\n",
                "        nrow = len(df[df[\"gene_diag\"]==i])\n",
                "        ds = ds / nrow * 100\n",
                "        stat_per_gene[i] = {}\n",
                "        stat_per_gene[i][\"n\"] = nrow\n",
                "        stat_per_gene[i][\"feature\"] = ds[ds>0].round().to_dict()\n",
                "    return stat_per_gene"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}